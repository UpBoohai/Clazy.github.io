---
layout:     post
title:      简单Logistic回归解决二分类问题
mathjax:    true
subtitle:   理论部分
date:       2020-02-12
author:     Clazy
cover: img/post-bg-sigmoid.jpg
catalog: true
tags: 机器学习
---



“喂”给Sigmoid函数的$$y$$的表达式不同，超平面的形状也就不同（$$y$$的表达式就是超平面的表达式）。所以，实际上Logistic回归可以拟合“非线性”的超平面（$$y$$的表达式为非线性函数）来解决二分类问题。

<!--break-->

这里，以“线性”超平面为例，来介绍简单Logistic回归如何解决二分类问题。

Logistic回归解决二分类问题，针对两种数据结构实际上有两种截然不同的优化思路。习惯上，我们把这两种数据结构划分为“分组数据”和“未分组数据”。在机器学习中，一般遇到的数据集都是所谓的“未分组数据”；而在统计分析中，很多教材也十分强调“分组数据”。下文中会分别介绍和解释两种不同的优化思路，但是只强调在实际中应用比较多的“未分组数据”。

## 分组数据

购买房屋的顾客记为1,没有购买的记为0,并按照年收入和受教育年限对顾客进行分组，形成下面的表格。

| 年收入（万元）$$x_1$$ | 受教育年限（年）$$x_2$$ | 签订意向书人数$$n$$ | 实际购房人数$$m$$ | 实际购房比例$$p=\frac{m} {n}$$ | 逻辑变换$$p'=ln(\frac{p} {1-p})$$ | 权重$$w=np(1-p)$$ |
| :-----------------: | :-------------------: | :---------------: | :-------------: | :--------------------------: | :-----------------------------: | :-------------: |
|         1.5         |           0           |        25         |        8        |             0.32             |              -0.75              |      5.44       |
|         2.5         |           1           |        32         |       13        |             0.41             |              -0.38              |      7.72       |
|         3.5         |           2           |        58         |       26        |             0.45             |              -0.21              |      14.35      |

对于这种分组数据，首先计算“1”所占的比重，即$$p=\frac{m} {n}=\frac{e^{y}} {1+e^{y}}=\frac{e^{\beta_0+\beta_1 x_1\beta_2 x_2}} {1+e^{\beta_0+\beta_1 x_1+\beta_2 x_2}}$$

这里$$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\varepsilon$$为一个线性函数，也就是一个线性超平面。

再对$$p$$进行“逻辑变换”：$$p'=ln(\frac{p} {1-p})=\beta_0+\beta_1 x_1+\beta_2 x_2=y$$

这样已知$$y,x_1,x_2$$可以对上式进行WLS（加权最小二乘）回归，拟合出最优参数$$\beta_0,\beta_1,\beta_2$$。

之所以进行WLS回归而不进行OLS（普通最小二乘）回归，是因为二值变量做因变量回归的残差具有异方差性，当$n$足够大时，$$\sigma(\varepsilon)\approx\frac{1} {np(1-p)}$$。所以要消除异方差，有$$y'=wy=w(\beta_0+\beta_1 x_1+\beta_2 x_2+\varepsilon)$$，再进行OLS估计。

进行OLS估计的优化函数为损失函数$$MSE=\frac{\Sigma_{i=1}^N(y'-\hat{y'})^2} {N}$$，机器学习中可用梯度下降法来优化参数。

得到最优参数之后可以计算出$$\hat{y'}$$进而计算出$$\hat{p}=\frac{e^\hat{y}} {1+e^\hat{y}}$$来预测在自变量$$x_1,x_2$$的条件下，$$y=1$$所占的比重（实际购房比例），或者解释为，在自变量$$x_1,x_2$$的条件下，$$y=1$$的平均值。

## 未分组数据（重要）

购买房屋的顾客记为1,没有购买的记为0。

| 性别$$x_1 $$ | 年龄（岁）$$x_2 $$ |  y   |
| :--------: | :--------------: | :--: |
|     0      |        18        |  0   |
|     0      |        21        |  1   |
|     1      |        23        |  1   |

这里$$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\varepsilon$$仍取为一个线性函数。

$$p=\frac{e^{y}} {1+e^{y}}$$为$$y=1 $$的概率，则$$y=0$$的概率为$$1-p$$。

那么，一个样本的$$y$$的概率函数合写为：$$P=p^y(1-p)^{1-y} \quad y=0,1$$

为了避免处理异方差，我们通常用极大似然估计来优化参数。

极大似然函数为：$$L=\prod_{i=1}^NP_i=p_i^{y_i}(1-p_i)^{1-y_i} $$

对似然函数取对数，得


$$
\begin{aligned}
lnL&=\Sigma_{i=1}^N[y_ilnp_i+(1-y_i)ln(1-p_i))]\\
&=\Sigma_{i=1}^N[y_iln\frac{p_i} {1-p_i}+ln(1-p_i)]
\end{aligned}
$$



因为$$p_i=\frac{e^{y_i}}{1+e^{y_i}}$$，代入上式可得：


$$
\begin{aligned}
lnL&=\Sigma_{i=1}^N[y_iln\frac{p_i} {1-p_i}+ln(1-p_i)]\\
&=\Sigma_{i=1}^N[y_i\times y_i+ln(1+e^{y_i})]\\
&=\Sigma_{i=1}^N[y_i\times (\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+\varepsilon)+ln(1+e^{\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+\varepsilon})]
\end{aligned}
$$


综上的对数似然函数就是我们的优化目标函数，这里要最大化对数似然函数，因而用梯度上升算法进行优化。

机器学习中，对数似然函数梯度的计算：


$$
\left(
\begin{aligned}
\frac{\partial lnL} {\partial \beta_0}&=\Sigma_{i=1}^N(y_i\times1-\frac{e^{y_i}\times 1} {1+e^{y_i}})=\Sigma_{i=1}^N[1 \times (y_i-\frac{e^{y_i}} {1+e^{y_i}})]\\
\frac{\partial lnL} {\partial \beta_1}&=\Sigma_{i=1}^N(y_i\times x_{1i}-\frac{e^{y_i}\times x_{1i}} {1+e^{y_i}})=\Sigma_{i=1}^N[x_{1i} \times (y_i-\frac{e^{y_i}} {1+e^{y_i}})]\\
\frac{\partial lnL} {\partial \beta_2}&=\Sigma_{i=1}^N(y_i\times x_{2i}-\frac{e^{y_i} \times x_{2i}} {1+e^{y_i}})=\Sigma_{i=1}^N[x_{2i} \times (y_i-\frac{e^{y_i}} {1+e^{y_i}})]\\
\end{aligned}
\right)
$$


根据上式推导，$$\beta_k^{(n)}=\beta_k^{(n-1)}+ \alpha \nabla_{\beta_k}$$



## 实例及代码实现

针对未分组数据，下面给出实例和Python代码(略)



